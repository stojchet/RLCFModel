\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}

\begin{document}

\section*{Thesis Proposal}
\textbf{Topic}: \href{https://arxiv.org/abs/2305.18341}{Tuning Models of Code with Compiler-Generated RL Feedback}
\textbf{Algorithm}: \href{https://arxiv.org/abs/2305.18290}{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\section*{Problem Statement}
State-of-the-art LLMs trained on code, although powerful, can sometimes make mistakes that a compiler can detect. LLM-generated code may produce non-terminating loops or use uninitialized variables or incorrect types. This problem can be framed as the model's output not aligned with the user’s desired outcome of compilable code. RL is in general quite unstable because of the number of hyperparameters to tune, non-stationarity in dynamic environments, the delicate balance between exploration and exploitation, alongside challenges in reward design and algorithm selection.

\section*{Solution}
In order to alleviate the misalignment between the model’s output code and code that is compilable, the Jani et. al. 2023 paper introduces a step before fine tuning, called course tuning which formulates the problem of writing code as an MDP. In this definition, the pre-trained model acts as a policy and we train a reward mode consisting of two parts: a compiler (that performs static analysis and returns a binary output (1 or -1)) and a discriminator (should predict whether the code given to it was generated by a model). The discriminator is represented by:

\[
g_w(x, y_0, y_1) \equiv \tanh(\text{MLP}(\text{CodeBERT}(x \circ y_0)) - \text{MLP}(\text{CodeBERT}(x \circ y_1)))
\]

Then to improve the policy, we plug the reward model and the policy in PPO. 

Another solution will be built on the first one, with one small difference: using DPO instead of PPO, which will help with the second problem: instability of RL. The main difference between the two methods is that DPO rephrases the loss of training the model from a loss wrt. to the reward, to a loss wrt. the model (or policy). DPO loss:

\[
\mathcal{L}_{DPO}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} [\log\sigma (\beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)})]
\]

In this case, we would still need the reward model as defined above, but instead of using it in an RL loop, it will provide us a vote on which solution is the one generated by the model, which we will consider the dispreferred solution. So the goal of the training will be trying to fool the discriminator that the LLM generated solution is the reference solution.

\section*{Experiments}
Validate that the proposed solution (one or two, as time permits) solves the specific problem, by comparing it to the baseline(s) using specific metrics (same as RLCF). Some metrics may be fraction of results that “compile”, similarity metrics like BLEU or ROUGE on which we compare:
The pre-trained model (e.g. phi1, starcoder1b, etc).
The same model after it is instruction-tuned.
The same model, but now coarse-tuned.
[optional] The same model, but fine-tuned.


\end{document}
